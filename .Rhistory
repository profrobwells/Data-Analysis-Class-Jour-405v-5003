#Delete blank rows
#myData %>% remove_empty("rows")
NYT4 %>% remove_empty("rows")
#Delete blank rows
#myData %>% remove_empty("rows")
NYT4 <- NYT4 %>% remove_empty("rows")
View(NYT4)
#Delete blank cells
#df[!(is.na(df$start_pc) | df$start_pc==""), ]
NYT4[!(is.na(NYT4$Start) | NYT4$Start==""), ]
#Delete blank cells
#df[!(is.na(df$start_pc) | df$start_pc==""), ]
NYT4 <- NYT4[!(is.na(NYT4$Start) | NYT4$Start==""), ]
View(NYT4)
colnames(NYT4)
setnames(NYT4, old = c("Text", "Start", "linenumber", "newarticle"), new = c("headlineX",'headline', "line",'article_nmbr'))
setnames(NYT4, old = c("Text", "Start", "linenumber", "newarticle"), new = c("headlineX", "headline", "line", "article_nmbr'))
)
)
()
setnames(NYT4, old = c("Text", "Start", "linenumber", "newarticle"), new = c("headlineX", "headline", "line", "article_nmbr"))
setnames(NYT4, old = c("Text", "Start", "linenumber", "newarticle"), new = c("headlineX", "headline", "line", "article_nmbr"))
setnames(NYT4, old = c('Text', 'Start', 'linenumber', 'newarticle'), new = c('headlineX', 'headline', 'line', 'article_nmbr'))
colnames(NYT4)
View(NYT4)
NYT4 <- select(headline, line, article_nmbr)
NYT4 <- select(NYT4, c(headline, line, article_nmbr))
View(NYT4)
NYT_tidy <- inner_join(NYT_Tidy_Headlines, NYT4, by =c("article_nmbr" = "article_nmbr"))
View(NYT_tidy)
colnames(NYT_tidy)[1] <- "Title"
colnames(NYT_tidy)[3] <- "Text"
head(NYT_tidy)
str(NYT_tidy)
#Tokenize the NYT table
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
NYT_token <- NYT_tidy %>%
filter(!str_detect(Text, '^"')) %>%
mutate(Text = str_replace_all(Text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
??unnest_tokens
#Tokenize the NYT table
library(tidytext)
NYT_token <- NYT_tidy %>%
filter(!str_detect(Text, '^"')) %>%
mutate(Text = str_replace_all(Text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
View(NYT_token)
CommonNYTWords <- NYT_token %>%
count(word, source) %>%
filter(sum(n) >= 5) %>%
spread(source, n, fill = 0) %>%
ungroup()
CommonNYTWords <- NYT_token %>%
count(word, Title) %>%
filter(sum(n) >= 5) %>%
spread(Title, n, fill = 0) %>%
ungroup()
View(CommonNYTWords)
CommonNYTWords <- NYT_token %>%
count(word) %>%
filter(sum(n) >= 5) %>%
spread(word, n, fill = 0) %>%
ungroup()
View(CommonNYTWords)
Collins2 <- read_csv(file.choose())
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- Collins2 %>%
filter(!str_detect(text, '^"')) %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
View(tweet_words)
CommonCollinsWords <- tweet_words %>%
count(word, source) %>%
filter(sum(n) >= 5) %>%
spread(source, n, fill = 0) %>%
ungroup()
View(CommonCollinsWords)
CommonNYTWords <- NYT_token %>%
count(word) %>%
filter(sum(n) >= 5) %>%
ungroup()
View(CommonNYTWords)
nrc <- sentiments %>%
filter(lexicon == "nrc") %>%
dplyr::select(word, sentiment)
NYT_sentiment <- CommonNYTWords %>%
count(word) %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, n) %>%
arrange(desc(n))
View(nrc)
??purrr
library(purrr)
NYT_sentiment <- CommonNYTWords %>%
count(word) %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, n) %>%
arrange(desc(n))
rlang::last_error()
#WITH NYT DATA
library(dplyr)
NYT_sentiment <- CommonNYTWords %>%
count(word) %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, n) %>%
arrange(desc(n))
library(tidyselect)
NYT_sentiment <- CommonNYTWords %>%
count(word) %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, n) %>%
arrange(desc(n))
View(tweet_words)
View(NYT_token)
totalwords <- NYT_token %>%
group_by(article_nmbr) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(article_nmbr, total_words)
NYT_sentiment <- CommonNYTWords %>%
count(word) %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, n) %>%
arrange(desc(n))
NYT_sentiment <- CommonNYTWords %>%
count(word) %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, n)
NYT_sentiment <- CommonNYTWords %>%
count(word) %>%
inner_join(nrc, by = "word")
View(NYT_sentiment)
NYT_sentiment <- CommonNYTWords %>%
count(word) %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, nn) %>%
arrange(desc(nn))
View(NYT_sentiment)
View(totalwords)
View(CommonCollinsWords)
View(nrc)
NYT_sentiment <- CommonNYTWords %>%
count(word)
View(NYT_sentiment)
NYT_sentiment <- CommonNYTWords %>%
count(n)
View(NYT_sentiment)
NYT_sentiment <- CommonNYTWords %>%
count(word)
View(NYT_sentiment)
View(CommonNYTWords)
NYT_sentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word")
View(NYT_sentiment)
NYT_sentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, n) %>%
arrange(desc(n))
View(NYT_sentiment)
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment) %>%
ungroup() %>%
complete(sentiment, fill = list(n = 0)) %>%
group_by(sentiment, total_words) %>%
summarize(words = sum(n)) %>%
arrange(desc(words)) %>%
ungroup()
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment) %>%
ungroup() %>%
complete(sentiment, fill = list(n = 0)) %>%
group_by(sentiment, word) %>%
summarize(words = sum(n)) %>%
arrange(desc(words)) %>%
ungroup()
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word")
View(total_NYTsentiment)
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment)
View(total_NYTsentiment)
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment) %>%
ungroup()
View(total_NYTsentiment)
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment) %>%
ungroup() %>%
complete(sentiment, fill = list(n = 0))
View(total_NYTsentiment)
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment) %>%
ungroup() %>%
complete(sentiment, fill = list(n = 0)) %>%
group_by(sentiment, nn)
View(total_NYTsentiment)
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment) %>%
ungroup() %>%
complete(sentiment, fill = list(n = 0)) %>%
group_by(sentiment, nn) %>%
summarize(words = sum(nn))
View(total_NYTsentiment)
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment) %>%
ungroup() %>%
complete(sentiment, fill = list(n = 0)) %>%
group_by(sentiment, nn) %>%
summarize(words = sum(nn)) %>%
arrange(desc(words)) %>%
ungroup()
View(total_NYTsentiment)
NYT5 <- NYT4 %>%
mutate(linenumber = row_number())
View(NYT5)
#Reindex the df
NYT4 <- NYT4 %>%
mutate(linenumber = row_number())
NYT4 <- select(NYT4, c(headline, linenumber, article_nmbr))
colnames(NYT4)[2] <- "line"
View(NYT_tidy)
NYT_token2 <- NYT_tidy %>%
unnest_tokens(word, Text, token = "regex", pattern = reg)
View(NYT_token2)
NYT_token2 <- NYT_tidy %>%
unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
View(NYT_token2)
NYT_token2 <- NYT_tidy %>%
filter(!str_detect(Text, '^"')) %>%
unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
View(NYT_token2)
NYT_token <- NYT_tidy %>%
filter(!str_detect(Text, '^"')) %>%
mutate(Text = str_replace_all(Text, "https://t.co/[A-Za-z\\d]+|&amp;+[New York Times]", "")) %>%
unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
View(NYT_token)
??str_replace_all
NYT_token <- NYT_tidy %>%
filter(!str_detect(Text, '^"')) %>%
mutate(Text = str_replace_all(Text, "https://t.co/[A-Za-z\\d]+|&amp;|[New York Times]", "")) %>%
unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
NYT_token <- NYT_tidy %>%
filter(!str_detect(Text, '^"')) %>%
mutate(Text = str_replace_all(Text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
CommonNYTWords <- NYT_token %>%
count(word) %>%
filter(sum(n) >= 5) %>%
ungroup()
View(CommonNYTWords)
View(NYT_token)
NYT_token$word2 <-
gsub("'","", NYT_token$word)
colnames(NYT_token)
NYT_token <- select c("Title", "article_nmbr", "line", "word2")
NYT_token <- select (NYT_token, c(Title, article_nmbr, line, word2))
colnames(NYT_token)[4] <- "word"
colnames(NYT_token)
CommonNYTWords <- NYT_token %>%
count(word) %>%
filter(sum(n) >= 5) %>%
ungroup()
View(CommonNYTWords)
View(CommonNYTWords)
colnames(NYT_tidy)
View(NYT2)
NYT <- read_lines("NYT1_26.txt")
NYT <- read_lines("NYT1_26.txt")
NYT <-as.data.frame(NYT)
View(NYT)
??is.na
NYT<- NYT[!is.na(NYT)]
NYT <-as.data.frame(NYT)
colnames(NYT)
colnames(NYT)[1] <- "Text"
NYT$Start <-
gsub("____________________________________________________________",
"STARTOFARTICLE", NYT$Text)
view(NYT)
View(NYT)
NYT2 <- NYT %>%
mutate(linenumber = row_number(),
newarticle = cumsum(str_detect(Start, regex("STARTOFARTICLE",
ignore_case = TRUE, na.rm=TRUE)))) %>%
ungroup()
#Delete blank cells
#df[!(is.na(df$start_pc) | df$start_pc==""), ]
NYT2 <- NYT2[!(is.na(NYT2$Start) | NYT2$Start==""), ]
#Delete metadata
NYT3 <- NYT2[!NYT2$Start=="" | NYT2$Start=="Author: ",]
View(NYT3)
#Delete metadata
#Example
## Sample Data
NO <- c(34, 42, 21, 3)
ARTICLE <- c('New York Times reports blah blah fake news',
'Financial Times blah blah',
'Fox News has been very nice to me',
'Newswire reports blah blah')
ARTICLE <- c('New York Times reports blah blah fake news',
'Financial Times blah blah',
'Fox News has been very nice to me',
'Newswire reports blah blah')
df <- data.frame(NO, ARTICLE)
View(df)
# Create List of Exclusion Phrases
fakenews <- c('New York Times', 'Newswire')
# Exclude
very.nice.to.me <- df[ !grepl(paste(fakenews, collapse="|"), df$ARTICLE),]
View(very.nice.to.me)
NYT3 <- NYT2
# Create List of Exclusion Phrases
cutmeta <- c('Author:', 'Publication Info', 'http://')
colnames(NYT3)
View(NYT3)
# Exclude
NYT3 <- df[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT3)
# Exclude
#very.nice.to.me <- df[ !grepl(paste(fakenews, collapse="|"), df$ARTICLE),]
NYT3 <- NYT2
View(NYT3)
cutmeta
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT4)
# Create List of Exclusion Phrases
cutmeta <- c('Author:', 'Publication Info', 'http://','Company:', 'Country of publication:', 'Dateline:', 'Document feature:',
'Document type:', 'Location:', 'Number of pages:', 'Place of publication:', 'Publication title:', 'Publication year:', 'Publisher:', 'Section:', 'Source type:', 'Subject:',
'Title:')
cutmeta <- c('Author:', 'Publication Info', 'http://',
'Company:', 'Country of publication:', 'Dateline:', 'Document feature:',
'Document type:', 'Location:', 'Number of pages:', 'Place of publication:',
'Publication title:', 'Publication year:', 'Publisher:', 'Section:',
'Source type:', 'Subject:', 'Title:')
cutmeta
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT4)
cutmeta <- c('Author:', 'Publication Info', 'http://',
'Company:', 'Country of publication:', 'Dateline:', 'Document feature:',
'Document type:', 'Location:', 'Number of pages:', 'Place of publication:',
'Publication title:', 'Publication year:', 'Publisher:', 'Section:',
'Source type:', 'Subject:', 'Title:', 'Company / organization: ', 'Credit:')
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT4)
NYT4 <- NYT4 %>%
mutate(linenumber = row_number())
colnames(NYT4)
cutmeta <- c('Author:', 'Publication Info', 'http://',
'Company:', 'Country of publication:', 'Dateline:', 'Document feature:',
'Document type:', 'Location:', 'Number of pages:', 'Place of publication:',
'Publication title:', 'Publication year:', 'Publisher:', 'Section:',
'Source type:', 'Subject:', 'Company / organization: ', 'Credit:')
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT4)
NYT_Tidy_Headlines <- NYT4 %>%
select(Start, newarticle) %>%
filter(str_detect(Start, "Title: "))
View(NYT_Tidy_Headlines)
View(NYT_Tidy_Headlines)
colnames(NYT4)
setnames(NYT4, old = c('Text', 'Start', 'linenumber', 'newarticle'), new = c('headlineX', 'headline', 'line', 'article_nmbr'))
colnames(NYT4)
NYT4 <- select(NYT4, c(headline, line, article_nmbr))
colnames(NYT4)
View(NYT4)
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT4)
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
# Exclude
#very.nice.to.me <- df[ !grepl(paste(fakenews, collapse="|"), df$ARTICLE),]
NYT3 <- NYT2
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT2)
cutmeta <- c('Author:', 'Publication Info', 'http://',
'Company:', 'Country of publication:', 'Dateline:', 'Document feature:',
'Document type:', 'Location:', 'Number of pages:', 'Place of publication:',
'Publication title:', 'Publication year:', 'Publisher:', 'Section:',
'Source type:', 'Subject:', 'Company / organization: ', 'Credit:')
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT4)
colnames(NYT3)
colnames(NYT2)
NYT2 <- NYT %>%
mutate(linenumber = row_number(),
newarticle = cumsum(str_detect(Start, regex("STARTOFARTICLE",
ignore_case = TRUE, na.rm=TRUE)))) %>%
ungroup()
colnames(NYT2)
#Delete blank cells
#df[!(is.na(df$start_pc) | df$start_pc==""), ]
NYT2 <- NYT2[!(is.na(NYT2$Start) | NYT2$Start==""), ]
colnames(NYT2)
# Exclude
#very.nice.to.me <- df[ !grepl(paste(fakenews, collapse="|"), df$ARTICLE),]
NYT3 <- NYT2
# Exclude
NYT4 <- NYT3[ !grepl(paste(cutmeta, collapse="|"), NYT3$Start),]
View(NYT4)
NYT_Tidy_Headlines <- NYT4 %>%
select(Start, newarticle) %>%
filter(str_detect(Start, "Title: "))
View(NYT_Tidy_Headlines)
colnames(NYT4)
setnames(NYT4, old = c('Text', 'Start', 'linenumber', 'newarticle'), new = c('x', 'text', 'line', 'article_nmbr'))
colnames(NYT4)
View(NYT4)
NYT4 <- select(NYT4, c(text, line, article_nmbr))
NYT4 <- NYT4 %>%
mutate(linenumber = row_number())
View(NYT_Tidy_Headlines)
setnames(NYT_Tidy_Headlines, old = c('Start','newarticle'), new = c('headline','article_nmbr'))
NYT_tidy <- inner_join(NYT_Tidy_Headlines, NYT4, by =c("article_nmbr" = "article_nmbr"))
View(NYT_tidy)
colnames(NYT_tidy)
NYT_tidy2 <- select(NYT_tidy, c(headline, article_nmbr, text, linenumber))
View(NYT_tidy2)
NYT_tidy <- select(NYT_tidy, c(headline, article_nmbr, text, linenumber))
colnames(NYT_tidy)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
NYT_token <- NYT_tidy %>%
filter(!str_detect(Text, '^"')) %>%
mutate(Text = str_replace_all(Text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, Text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
NYT_token <- NYT_tidy %>%
filter(!str_detect(text, '^"')) %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
View(NYT_token)
NYT_token$word2 <-
gsub("'","", NYT_token$word)
NYT_token <- select (NYT_token, c(Title, article_nmbr, line, word2))
NYT_token
colnames(NYT_token)
NYT_token <- select (NYT_token, c(headline, article_nmbr, linenumber, word2))
colnames(NYT_token)[4] <- "word"
colnames(NYT_token)
CommonNYTWords <- NYT_token %>%
count(word) %>%
filter(sum(n) >= 5) %>%
ungroup()
View(CommonNYTWords)
totalwords <- NYT_token %>%
group_by(article_nmbr) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(article_nmbr, total_words)
View(totalwords)
NYT_sentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
select(word, sentiment, n) %>%
arrange(desc(n))
View(NYT_sentiment)
#Export output this file to a CSV or Excel  write.csv or write.excel
write.csv(NYT_sentiment,"NYT_sentiment.csv")
total_NYTsentiment <- CommonNYTWords %>%
inner_join(nrc, by = "word") %>%
count(sentiment) %>%
ungroup() %>%
complete(sentiment, fill = list(n = 0)) %>%
group_by(sentiment, nn) %>%
summarize(words = sum(nn)) %>%
arrange(desc(words)) %>%
ungroup()
View(total_NYTsentiment)
colnames(total_NYTsentiment)
total_NYTsentiment2 <- select (total_NYTsentiment, c(-nn))
View(total_NYTsentiment2)
total_NYTsentiment <- select (total_NYTsentiment, c(-nn))
#Export output this file to a CSV or Excel  write.csv or write.excel
write.csv(total_NYT_sentiment,"total_NYT_sentiment.csv")
#Export output this file to a CSV or Excel  write.csv or write.excel
write.csv(total_NYTsentiment,"total_NYTsentiment.csv")
View(total_NYTsentiment2)
write.csv(CommonNYTWords,"CommonNYTWords.csv")
write.csv(NYT_tidy,"NYT_tidy.csv")
